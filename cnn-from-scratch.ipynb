{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-20T01:01:05.309906Z","iopub.execute_input":"2021-11-20T01:01:05.310271Z","iopub.status.idle":"2021-11-20T01:01:05.323034Z","shell.execute_reply.started":"2021-11-20T01:01:05.310231Z","shell.execute_reply":"2021-11-20T01:01:05.322231Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:01:00.158984Z","iopub.execute_input":"2021-11-19T22:01:00.159791Z","iopub.status.idle":"2021-11-19T22:01:00.409998Z","shell.execute_reply.started":"2021-11-19T22:01:00.159713Z","shell.execute_reply":"2021-11-19T22:01:00.407976Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fer2013-dataset/fer2013.csv\")\n\n\nmatrix = np.zeros((48, 48), dtype=np.uint8)\n\ntrain_images = []\ntest_images = []\ntrain_labels = []\ntest_labels = []\n\ndata_type = df[\"Usage\"].value_counts()\ndata_type = data_type.to_dict()\ndata_type[\"Test\"] = data_type[\"PrivateTest\"]+data_type[\"PublicTest\"]\ndel data_type[\"PrivateTest\"]\ndel data_type[\"PublicTest\"]\nprint(data_type)\n\ndef atoi(s):\n  n = 0\n  for i in s:\n    n = n * 10 + ord(i) - ord(\"0\")\n  return n\n\n\nfor value in tqdm(range(len(df))):\n  pixels = df['pixels'][value]\n  pixels_split = pixels.split()\n\n  for values in range(2304):\n    x_index = values // 48\n    y_index = values % 48\n    matrix[x_index][y_index] = atoi(pixels_split[values])\n\n  if value<=data_type['Training']:\n    train_images.append(matrix)\n    train_labels.append(df[\"emotion\"][value])\n  else:\n    test_images.append(matrix)\n    test_labels.append(df[\"emotion\"][value])\n\ntrain_images = np.array(train_images)\ntest_images = np.array(test_images)\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\nprint(train_images)\nprint(test_images)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:01:00.412042Z","iopub.execute_input":"2021-11-19T22:01:00.412599Z","iopub.status.idle":"2021-11-19T22:03:44.823663Z","shell.execute_reply.started":"2021-11-19T22:01:00.412555Z","shell.execute_reply":"2021-11-19T22:03:44.822690Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class conv:\n    def __init__(self, number_of_filters, filter_dim):\n        self.number_of_filters = number_of_filters\n        self.filter_dim = filter_dim\n        self.conv_filters = np.random.randn(number_of_filters, filter_dim, filter_dim)/(filter_dim * filter_dim)\n\n    def image_region(self, image):\n        height, width = image.shape\n        self.image = image\n        for j in range(height - self.filter_dim + 1):\n            for k in range(width - self.filter_dim + 1):\n                image_patch = image[ j : (j + self.filter_dim), k : (k + self.filter_dim)]\n                yield image_patch, j, k\n\n    def forward_propagation(self, image):\n        height, width = image.shape\n        conv_out = np.zeros((height - self.filter_dim + 1, width - self.filter_dim + 1, self.number_of_filters))\n        for image_patch, i, j in self.image_region(image):\n            conv_out[i, j] = np.sum(image_patch * self.conv_filters, axis = (1,2))\n        return conv_out\n\n    def back_propagation(self, dl_dout, learning_rate):\n        dl_df_params = np.zeros(self.conv_filters.shape)\n        for image_patch, i, j in self.image_region(self.image):\n            for k in range(self.number_of_filters):\n                dl_df_params[k] += image_patch*dl_dout[i, j, k]\n\n            self.conv_filters -= learning_rate*dl_df_params\n            return dl_df_params","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.824932Z","iopub.execute_input":"2021-11-19T22:03:44.825149Z","iopub.status.idle":"2021-11-19T22:03:44.837124Z","shell.execute_reply.started":"2021-11-19T22:03:44.825122Z","shell.execute_reply":"2021-11-19T22:03:44.836296Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class max_pool:\n    def __init__(self, filter_dim):\n        self.filter_dim = filter_dim\n\n    def image_region(self, image):\n        new_height = image.shape[0] // self.filter_dim\n        new_width = image.shape[1] // self.filter_dim\n        self.image = image\n\n        for i in range(new_height):\n            for j in range(new_width):\n                image_patch = image[(i*self.filter_dim) :(i*self.filter_dim + self.filter_dim), (j*self.filter_dim) : (j*self.filter_dim + self.filter_dim)]\n                yield image_patch, i, j\n\n    def forward_propagation(self, image):\n        height, width, number_of_filters = image.shape\n        output = np.zeros((height // self.filter_dim, width // self.filter_dim, number_of_filters))\n\n        for image_patch, i, j in self.image_region(image):\n            output[i, j] = np.amax(image_patch, axis = (0,1))\n\n        return output\n\n    def back_propagation(self, dl_dout):\n        dl_dmax_pool = np.zeros(self.image.shape)\n        for image_patch, i, j in self.image_region(self.image):\n            height, width, number_of_filters = image_patch.shape\n            maximum_val = np.amax(image_patch, axis=(0,1))\n\n            for i1 in range(height):\n                for j1 in range(width):\n                    for k1 in range(number_of_filters):\n                        if image_patch[i1, j1, k1] == maximum_val[k1]:\n                            dl_dmax_pool[i*self.filter_dim + i1, j*self.filter_dim + j1, k1] = dl_dout[i, j, k1]\n\n            return dl_dmax_pool","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.839250Z","iopub.execute_input":"2021-11-19T22:03:44.839459Z","iopub.status.idle":"2021-11-19T22:03:44.853401Z","shell.execute_reply.started":"2021-11-19T22:03:44.839435Z","shell.execute_reply":"2021-11-19T22:03:44.852678Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class softmax:\n    def __init__(self, input_node, softmax_node):\n        self.weight = np.random.randn(input_node, softmax_node)/input_node\n        self.bias = np.zeros(softmax_node)\n\n    def forward_propagation(self, image):\n        self.orig_im_shape = image.shape\n        image_modified = image.flatten()\n        self.modified_input = image_modified\n        output_val = np.dot(image_modified, self.weight) + self.bias\n        self.out = output_val\n        exp_out = np.exp(output_val)\n\n        return exp_out/np.sum(exp_out, axis = 0)\n\n    def back_propagation(self, dl_dout, learning_rate):\n        for i, grad in enumerate(dl_dout):\n            if grad == 0:\n                continue\n\n            transformation_eq = np.exp(self.out)\n            s_total = np.sum(transformation_eq)\n\n            dy_dz = -transformation_eq[i]*transformation_eq / (s_total ** 2)\n            dy_dz[i] = transformation_eq[i] * (s_total - transformation_eq[i]) / (s_total ** 2)\n\n            dz_dw = self.modified_input\n            dz_db = 1\n            dz_d_inp = self.weight\n\n            dl_dz = grad * dy_dz\n\n            dl_dw = dz_dw[np.newaxis].T @ dl_dz[np.newaxis]\n            dl_db = dl_dz * dz_db\n            dl_d_inp = dz_d_inp @ dl_dz\n\n        self.weight -= learning_rate * dl_dw\n        self.bias -= learning_rate * dl_db\n\n        return dl_d_inp.reshape(self.orig_im_shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.855094Z","iopub.execute_input":"2021-11-19T22:03:44.855907Z","iopub.status.idle":"2021-11-19T22:03:44.869371Z","shell.execute_reply.started":"2021-11-19T22:03:44.855864Z","shell.execute_reply":"2021-11-19T22:03:44.868782Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"convo = conv(10, 7)\npool = max_pool(4)\nsoft_max = softmax(10*10*10, 7)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.870577Z","iopub.execute_input":"2021-11-19T22:03:44.870814Z","iopub.status.idle":"2021-11-19T22:03:44.883161Z","shell.execute_reply.started":"2021-11-19T22:03:44.870788Z","shell.execute_reply":"2021-11-19T22:03:44.882548Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def cnn_forward_prop(image, label):\n    out_p = convo.forward_propagation((image / 255) - 0.5)\n    out_p = pool.forward_propagation(out_p)\n    out_p = soft_max.forward_propagation(out_p)\n\n    cross_ent_loss = -np.log(out_p[label])\n    accuracy_eval = 1 if np.argmax(out_p) == label else 0\n\n    return out_p, cross_ent_loss, accuracy_eval\n\ndef training_cnn(image, label, learn_rate=0.001):\n\n    out, loss, acc = cnn_forward_prop(image, label)\n\n    gradient = np.zeros(7)\n    gradient[label] = -1 / out[label]\n\n    grad_back = soft_max.back_propagation(gradient, learn_rate)\n    grad_back = pool.back_propagation(grad_back)\n    grad_back = convo.back_propagation(grad_back, learn_rate)\n\n    return loss, acc","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.884104Z","iopub.execute_input":"2021-11-19T22:03:44.884802Z","iopub.status.idle":"2021-11-19T22:03:44.894803Z","shell.execute_reply.started":"2021-11-19T22:03:44.884755Z","shell.execute_reply":"2021-11-19T22:03:44.893962Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for epoch1 in range(6):\n    print('Epoch %d ----> ' %(epoch1 + 1))\n\n    shuffle_data = np.random.permutation(len(train_images))\n    train_images = train_images[shuffle_data]\n    train_labels = train_labels[shuffle_data]\n\n    loss = 0\n    num_correct = 0\n\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n        if i % 100 == 0:\n            print(\"%d steps out of 100 steps: Average Loss %.3f and Accuracy: %d%%\" %(i+1, loss / 100, num_correct))\n            loss = 0\n            num_correct = 0\n\n        l1, accu = training_cnn(im, label)\n        loss += l1\n        num_correct += accu\n\nprint(\"Testing Phase\")\nloss = 0\nnum_correct = 0\n\nfor im, label in zip(test_images, test_labels):\n    _, l1, accu = cnn_forward_prop(im, label)\n    loss += l1\n    num_correct += accu\n\nnum_tests = len(test_images)\nprint('Test Loss:', loss / num_tests)\nprint('Test Accuracy:', num_correct / num_tests)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:03:44.896836Z","iopub.execute_input":"2021-11-19T22:03:44.897385Z","iopub.status.idle":"2021-11-19T23:13:53.756611Z","shell.execute_reply.started":"2021-11-19T22:03:44.897344Z","shell.execute_reply":"2021-11-19T23:13:53.755198Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(test_images)\nprint(test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T23:24:50.993986Z","iopub.execute_input":"2021-11-19T23:24:50.994762Z","iopub.status.idle":"2021-11-19T23:24:51.000719Z","shell.execute_reply.started":"2021-11-19T23:24:50.994724Z","shell.execute_reply":"2021-11-19T23:24:50.999831Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def predict(x):\n    new_image = []\n    loss_array = []\n    accuracy_array = []\n    data_dict = dict()\n    emotionss = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n    for data in lab:\n        new_image.append(x[10000])\n    print(new_image)\n    for im, label in zip(new_image, emotionss.keys()):\n        loss = 0\n        num_correct = 0\n        _, l1, accu = cnn_forward_prop(im, label)\n        loss += l1\n        num_correct += accu\n        loss_array.append(loss / 100)\n        accuracy_array.append(num_correct)\n        data_dict[label] = num_correct\n        \n        \n    print(accuracy_array)\n    print(data_dict)\n    print(emotionss[max(data_dict, key=data_dict.get)])\n    \npredict(train_images)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:50:06.264675Z","iopub.execute_input":"2021-11-20T00:50:06.265036Z","iopub.status.idle":"2021-11-20T00:50:06.460614Z","shell.execute_reply.started":"2021-11-20T00:50:06.265000Z","shell.execute_reply":"2021-11-20T00:50:06.459752Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef predict(x):\n    new_image = []\n    loss_array = []\n    accuracy_array = []\n    new_images = []\n    data_dict = dict()\n    lab = [0, 1, 2, 3, 4, 5, 6]\n    emotionss = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n    for data in lab:\n        new_image.append(x[500])\n    print(new_image)\n    for im, label in zip(new_image, lab):\n        loss = 0\n        num_correct = 0\n        out, l1, accu = cnn_forward_prop(im, label)\n        loss += l1\n        num_correct += accu\n        \n    print(out)\n    for data in range(len(out)):\n        value = lab[data]\n        data_dict[value] = out[data]  \n    print(data_dict)\n    print(emotionss[max(data_dict, key=data_dict.get)])\n    \n    \npredict(test_images)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-20T00:35:02.579318Z","iopub.execute_input":"2021-11-20T00:35:02.579853Z","iopub.status.idle":"2021-11-20T00:35:02.755845Z","shell.execute_reply.started":"2021-11-20T00:35:02.579815Z","shell.execute_reply":"2021-11-20T00:35:02.754966Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing import image\n\nimg = image.load_img(\"/kaggle/input/sample-image/im0.png\",target_size = (48,48),color_mode = \"grayscale\")\nimg = np.array(img)\nplt.imshow(img)\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:01:28.700765Z","iopub.execute_input":"2021-11-20T01:01:28.701496Z","iopub.status.idle":"2021-11-20T01:01:28.936248Z","shell.execute_reply.started":"2021-11-20T01:01:28.701436Z","shell.execute_reply":"2021-11-20T01:01:28.935157Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)\nimg = img.reshape(1,48,48)\nimg","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:04:36.688998Z","iopub.execute_input":"2021-11-20T01:04:36.689292Z","iopub.status.idle":"2021-11-20T01:04:36.696510Z","shell.execute_reply.started":"2021-11-20T01:04:36.689259Z","shell.execute_reply":"2021-11-20T01:04:36.695548Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"def predict(x):\n    new_image = []\n    loss_array = []\n    accuracy_array = []\n    data_dict = dict()\n    emotionss = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n    for data in emotionss.keys():\n        new_image.append(x[0])\n    print(new_image)\n    for im, label in zip(new_image, emotionss.keys()):\n        loss = 0\n        num_correct = 0\n        _, l1, accu = cnn_forward_prop(im, label)\n        loss += l1\n        num_correct += accu\n        loss_array.append(loss / 100)\n        accuracy_array.append(num_correct)\n        data_dict[label] = num_correct\n        \n        \n    print(accuracy_array)\n    print(data_dict)\n    print(emotionss[max(data_dict, key=data_dict.get)])\n    \npredict(img)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T01:06:30.481857Z","iopub.execute_input":"2021-11-20T01:06:30.482591Z","iopub.status.idle":"2021-11-20T01:06:30.661681Z","shell.execute_reply.started":"2021-11-20T01:06:30.482542Z","shell.execute_reply":"2021-11-20T01:06:30.660793Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}